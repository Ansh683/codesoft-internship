# -*- coding: utf-8 -*-
"""image_captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UOyLO_CKqpKDs1SndpQZIl0lTPnDlZ2P
"""



import torch
import torch.nn as nn
import torchvision.models as models
from torch.nn.utils.rnn import pack_padded_sequence

class Vocabulary(object):
    def __init__(self):
        self.word2idx = {}
        self.idx2word = {}
        self.idx = 0
        self.add_word('<pad>')
        self.add_word('<start>')
        self.add_word('<end>')
        self.add_word('<unk>')

    def add_word(self, word):
        if word not in self.word2idx:
            self.word2idx[word] = self.idx
            self.idx2word[self.idx] = word
            self.idx += 1

    def __call__(self, word):
        return self.word2idx.get(word, self.word2idx['<unk>'])

    def __len__(self):
        return self.idx


class EncoderCNN(nn.Module):
    def __init__(self, embed_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        modules = list(resnet.children())[:-1]
        self.resnet = nn.Sequential(*modules)
        self.linear = nn.Linear(resnet.fc.in_features, embed_size)
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)

    def forward(self, images):
        with torch.no_grad():
            features = self.resnet(images)
        features = features.reshape(features.size(0), -1)
        features = self.bn(self.linear(features))
        return features


class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, eos_id, max_seq_length=20):
        super(DecoderRNN, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, vocab_size)
        self.max_seq_length = max_seq_length
        self.eos_id = eos_id

    def forward(self, features, captions, lengths):
        embeddings = self.embed(captions)
        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)
        lengths = [l + 1 for l in lengths]
        packed = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)
        packed_out, _ = self.lstm(packed)
        outputs = self.linear(packed_out.data)
        return outputs

    def sample(self, features, states=None):
        sampled_ids = []
        inputs = features.unsqueeze(1)
        with torch.no_grad():
            for _ in range(self.max_seq_length):
                hiddens, states = self.lstm(inputs, states)
                outputs = self.linear(hiddens.squeeze(1))
                _, predicted = outputs.max(1)
                pid = predicted.item()
                sampled_ids.append(pid)
                if pid == self.eos_id:
                    break
                inputs = self.embed(predicted).unsqueeze(1)
        return sampled_ids


if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    embed_size = 256
    hidden_size = 512
    num_layers = 1

    vocab = Vocabulary()
    words = ['a', 'cat', 'dog', 'is', 'playing', 'in', 'the', 'park', 'water', 'jumping', 'ball']
    for w in words:
        vocab.add_word(w)
    eos_id = vocab('<end>')

    encoder = EncoderCNN(embed_size).to(device)
    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers, eos_id).to(device)

    encoder.eval()
    decoder.eval()

    dummy_image = torch.randn(1, 3, 224, 224).to(device)
    with torch.no_grad():
        features = encoder(dummy_image)

    sampled_ids = decoder.sample(features)

    sentence = []
    for wid in sampled_ids:
        word = vocab.idx2word.get(wid, '<unk>')
        if word == '<end>':
            break
        if word not in ('<start>', '<pad>'):
            sentence.append(word)
    final_caption = ' '.join(sentence)
    print("Generated Caption:", final_caption)